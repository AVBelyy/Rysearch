{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 279,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import regex\n",
    "import collections\n",
    "from lxml import html\n",
    "from pymongo import MongoClient\n",
    "from urllib.request import urlopen\n",
    "from urllib.error import HTTPError\n",
    "from sklearn.pipeline import Pipeline\n",
    "from parsers.text_utils import DefaultTextProcessor, DefaultDocumentProcessor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Загрузка страниц с веб-сайта elementy.ru"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "maybe = lambda f, x: f(x) if x else None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def process_html(text):\n",
    "    return text.replace(\"\\xa0\", \" \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def process_tag(text):\n",
    "    return regex.sub(\"\\s\", \"_\", process_html(text).strip()).lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parsed 98/100 pages\n",
      "Parsed 198/200 pages\n",
      "Parsed 296/300 pages\n",
      "Parsed 392/400 pages\n",
      "Parsed 491/500 pages\n",
      "Parsed 589/600 pages\n",
      "Parsed 686/700 pages\n",
      "Parsed 784/800 pages\n",
      "Parsed 881/900 pages\n",
      "Parsed 981/1000 pages\n",
      "Parsed 1079/1100 pages\n",
      "Parsed 1176/1200 pages\n",
      "Parsed 1273/1300 pages\n",
      "Parsed 1370/1400 pages\n",
      "Parsed 1466/1500 pages\n",
      "Parsed 1566/1600 pages\n",
      "Parsed 1658/1700 pages\n",
      "Parsed 1752/1800 pages\n",
      "Parsed 1848/1900 pages\n",
      "Parsed 1946/2000 pages\n",
      "Parsed 2038/2100 pages\n",
      "Parsed 2129/2200 pages\n",
      "Parsed 2223/2300 pages\n",
      "CPU times: user 45.4 s, sys: 3.45 s, total: 48.9 s\n",
      "Wall time: 28min 17s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# Парсинг всех страниц\n",
    "pages_ids = list(range(431231, 433629))\n",
    "pages = []\n",
    "\n",
    "for i, page_id in enumerate(pages_ids, 1):\n",
    "    try:\n",
    "        page_url = \"http://elementy.ru/nauchno-populyarnaya_biblioteka/%d/\" % page_id\n",
    "        page = html.parse(urlopen(page_url))\n",
    "\n",
    "        title = process_html(page.findtext(\"//h1\"))\n",
    "        tags = list(map(lambda p: process_tag(p.text),\n",
    "                        page.findall(\"//div[@class='mb itemhead newslist']/div/a\")[1:-1]))\n",
    "        article = page.find(\"//div[@class='itemblock']/div[@class='memo']\")\n",
    "\n",
    "        summary = maybe(process_html, article.findtext(\"./p[@class='Intro']\"))\n",
    "        text = []\n",
    "        content_flag = False\n",
    "        for elem in article.iterfind(\"p\"):\n",
    "            if len(elem.classes) > 0:\n",
    "                continue\n",
    "            # TODO: filter wrong paragraphs\n",
    "            # TODO: can also be non-paragraphs (h3, ol, etc)\n",
    "            text.append(process_html(elem.text_content()))\n",
    "        text = \"\\n\\n\".join(text)\n",
    "        \n",
    "        pages.append((page_id, title, tags, summary, text))\n",
    "    except Exception:\n",
    "        pass\n",
    "    \n",
    "    if i % 100 == 0:\n",
    "        print(\"Parsed %d/%d pages\" % (len(pages), i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 267,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2300"
      ]
     },
     "execution_count": 267,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(pages)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Парсинг"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 292,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "stop_words = open(\"../datasets/elementy/stopwords.txt\").read().split()\n",
    "rare_words = open(\"../datasets/elementy/rarewords.txt\").read().split()\n",
    "stop_lemmas = set(stop_words).union(set(rare_words))\n",
    "doc_pipeline = Pipeline([\n",
    "    (\"text-processor\",     DefaultTextProcessor(token_pattern=\"(?u)\\\\b\\\\p{L}+\\\\b\")),\n",
    "    (\"document-processor\", DefaultDocumentProcessor(stop_lemmas=stop_lemmas)),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 293,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1.48 s, sys: 19 ms, total: 1.5 s\n",
      "Wall time: 1.52 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# TODO: вынести разнесение токенов по двум модальностям (MOD и MOD_habr) в отдельный модуль\n",
    "\n",
    "pn_vocab = {\"text\": set(), \"flat_tag\": set()}\n",
    "\n",
    "for doc in open(\"../datasets/postnauka/postnauka.txt\"):\n",
    "    tokens = doc.split()\n",
    "    for token in tokens[1:]:\n",
    "        if token.startswith(\"|\"):\n",
    "            cur_mod = token[1:]\n",
    "        else:\n",
    "            if cur_mod == \"text\" or cur_mod == \"flat_tag\":\n",
    "                pn_vocab[cur_mod].add(token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 294,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "44995"
      ]
     },
     "execution_count": 294,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(pn_vocab[\"text\"]) + len(pn_vocab[\"flat_tag\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 295,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "client = MongoClient()\n",
    "out_collection = client[\"datasets\"][\"elementy\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 296,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Written 500 pages\n",
      "Written 1000 pages\n",
      "Written 1500 pages\n",
      "Written 2000 pages\n",
      "CPU times: user 38.1 s, sys: 612 ms, total: 38.7 s\n",
      "Wall time: 2min 54s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "with open(\"../datasets/elementy/elementy.txt\", \"w\") as vw_file:\n",
    "    for i, page in enumerate(pages, 1):\n",
    "        page_id, title, tags, summary, text = page\n",
    "        doc = {}\n",
    "        doc[\"_id\"] = \"elem_%d\" % page_id\n",
    "        doc[\"title\"] = title\n",
    "        doc[\"url\"] = \"http://elementy.ru/nauchno-populyarnaya_biblioteka/%d/\" % page_id\n",
    "        doc[\"modalities\"] = {\"text_elem\": [], \"text\": [], \"flat_tag_elem\": [], \"flat_tag\": []}\n",
    "        modalities = doc_pipeline.fit_transform(text)\n",
    "        for token in modalities[\"text\"]:\n",
    "            if token in pn_vocab[\"text\"]:\n",
    "                doc[\"modalities\"][\"text\"].append(token)\n",
    "            else:\n",
    "                doc[\"modalities\"][\"text\"].append(token)\n",
    "                doc[\"modalities\"][\"text_elem\"].append(token)\n",
    "        for token in tags:\n",
    "            if token in pn_vocab[\"flat_tag\"]:\n",
    "                doc[\"modalities\"][\"flat_tag\"].append(token)\n",
    "            else:\n",
    "                doc[\"modalities\"][\"flat_tag\"].append(token)\n",
    "                doc[\"modalities\"][\"flat_tag_elem\"].append(token)\n",
    "        doc[\"summary\"] = summary\n",
    "        doc[\"markdown\"] = text\n",
    "        # Фильтрация коротких документов из Элементов\n",
    "        if len(doc[\"modalities\"][\"text\"]) > 100:\n",
    "            # Записать в Vowpal Wabbit\n",
    "            modalities_str = \" \".join(map(lambda p: \"|%s %s\" % (p[0],\n",
    "                             \" \".join(map(lambda t: \"_\".join(t.split()), p[1]))), doc[\"modalities\"].items()))\n",
    "            vw_file.write(\"%s %s\\n\" % (doc[\"_id\"], modalities_str))\n",
    "            # Записать в MongoDB\n",
    "            out_collection.insert_one(doc)\n",
    "        if i % 500 == 0:\n",
    "            print(\"Written %d pages\" % i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Фильтрация слов с низким DF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 285,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 500 pages\n",
      "Processed 1000 pages\n",
      "Processed 1500 pages\n",
      "Processed 2000 pages\n",
      "CPU times: user 32.1 s, sys: 214 ms, total: 32.4 s\n",
      "Wall time: 2min 50s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "word_counter = collections.defaultdict(set)\n",
    "\n",
    "for i, page in enumerate(pages, 1):\n",
    "    page_id, _, _, _, text = page\n",
    "    modalities = doc_pipeline.fit_transform(text)\n",
    "    for word in modalities[\"text\"]:\n",
    "        word_counter[word].add(page_id)\n",
    "    if i % 500 == 0:\n",
    "        print(\"Processed %d pages\" % i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 286,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "words = list(word_counter.items())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 287,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "79946"
      ]
     },
     "execution_count": 287,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(word_counter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 288,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "rare_words = set(map(lambda p: p[0], filter(lambda p: len(p[1]) <= 1, words)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 289,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "39494\n",
      "0.49400845570760266\n"
     ]
    }
   ],
   "source": [
    "print(len(rare_words))\n",
    "print(len(rare_words) / len(words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 291,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "388252"
      ]
     },
     "execution_count": 291,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "open(\"../datasets/elementy/rarewords.txt\", \"w\").write(\"\\n\".join(rare_words))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "---"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  },
  "latex_envs": {
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 0
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
