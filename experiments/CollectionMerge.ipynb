{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Эксперимент по вливанию коллекции в тематическую модель"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=DeprecationWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import os\n",
    "import sys\n",
    "import glob\n",
    "import json\n",
    "import urllib\n",
    "import pymongo\n",
    "import itertools\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import artm\n",
    "import hierarchy_utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "db = pymongo.MongoClient()[\"datasets\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "prefix_to_col_map = {\"pn\": \"postnauka\", \"habr\": \"habrahabr\"}\n",
    "\n",
    "def get_document(doc_id, with_markdown=False):\n",
    "    fields = {\"_id\": 1, \"title\": 1, \"modalities\": 1}\n",
    "    if with_markdown:\n",
    "        fields[\"markdown\"] = 1\n",
    "    prefix, _ = doc_id.split(\"_\", 1)\n",
    "    col_name = prefix_to_col_map[prefix]\n",
    "    return db[col_name].find_one({\"_id\": doc_id}, fields)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Шаг 1 -- получение ранжированного списка документов для доливания\n",
    "\n",
    "reduced_dataset = True\n",
    "shuffled_dataset = False\n",
    "\n",
    "docs_ids = list(map(lambda r: r[\"_id\"], db[\"postnauka\"].find({}, {\"_id\": 1})))\n",
    "\n",
    "D_a = len(docs_ids)\n",
    "\n",
    "clf_output = pd.read_csv(\"classifier_output.csv\")\n",
    "clf_output.columns = [\"id\", \"proba\"]\n",
    "clf_output = clf_output.set_index(\"id\")[\"proba\"].sort_values(ascending=False)\n",
    "\n",
    "if reduced_dataset:\n",
    "    reduced_docs_ids = list(map(str.strip, open(\"reduced_habr_docs_ids.txt\")))\n",
    "    clf_output = clf_output.loc[reduced_docs_ids]\n",
    "if shuffled_dataset:\n",
    "    np.random.seed(42)\n",
    "    clf_output = clf_output.sample(frac=1)\n",
    "\n",
    "docs_ids += list(clf_output.index)\n",
    "\n",
    "D_b = len(docs_ids) - D_a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2976, 20000)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "D_a, D_b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Шаг 2 -- разбиение списка на батчи для итеративного доливания\n",
    "\n",
    "# Будем пользоваться законом сложного процента: размер i-ого батча будет считаться как размер (i-1)-ого батча + p%.\n",
    "# Тогда количество итераций n, необходимых для вливания коллекции размера D_b в коллекцию размера D_a, удовлетворяет:\n",
    "# D_a * (1 + p)^n = D_a + D_b\n",
    "# n = ln((D_a + D_b) / D_a) / ln(1 + p)\n",
    "\n",
    "p = 0.1\n",
    "batches = [docs_ids[:D_a]]\n",
    "batch_pos = D_a\n",
    "\n",
    "while batch_pos < len(docs_ids):\n",
    "    new_batch_pos = int((1 + p) * batch_pos)\n",
    "    batch = docs_ids[batch_pos:new_batch_pos]\n",
    "    batches.append(batch)\n",
    "    batch_pos = new_batch_pos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(23, 22976)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(batches), sum(map(len, batches))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Параметры ARTM модели\n",
    "\n",
    "# Общий random seed\n",
    "artm_seed = 42\n",
    "\n",
    "# Веса всех модальностей\n",
    "class_ids0 = {\"text\": 1.0, \"flat_tag\": 100.0, \"text_habr\": 1.0, \"flat_tag_habr\": 1.0}\n",
    "class_ids1 = {\"text\": 1.0, \"flat_tag\": 1.0,   \"text_habr\": 1.0, \"flat_tag_habr\": 1.0}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Параметры алгоритма CollectionMerge\n",
    "\n",
    "modalities_to_use = {\"text\", \"text_habr\", \"flat_tag\", \"flat_tag_habr\"}\n",
    "vw_path = \"batch_vw.txt\"\n",
    "artm_cooc_path = \"merged_cooc.txt\"\n",
    "artm_vocab_path = \"merged_vocab.txt\"\n",
    "artm_batch_path = \"merged_batches/\"\n",
    "tmp_artm_batch_path = \"./\"\n",
    "\n",
    "cooc_window_size = 10\n",
    "ppmi_func = lambda n_uv, n_u, n_v, n: max(0, np.log(n_uv * n / n_u / n_v))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def build_level1(topics_cnt, level0, dictionary, batch_vectorizer, scores_list):\n",
    "    # Названия тем\n",
    "    topic_names1 = [\"topic_%d\" % i for i in range(topics_cnt)]\n",
    "\n",
    "    # Список регуляризаторов\n",
    "    regularizers_list1 = []\n",
    "    regularizers_list1.append(artm.DecorrelatorPhiRegularizer(name=\"DecorrPhiReg1\",\n",
    "                              topic_names=topic_names1, tau=50000))\n",
    "\n",
    "    # Создадим ARTM модель (второй уровень)\n",
    "    level1 = hierarchy_utils.ARTM_Level(level0, phi_batch_weight=10.0**3, \n",
    "                                        topic_names=topic_names1,\n",
    "                                        class_ids=class_ids1,\n",
    "                                        regularizers=regularizers_list1,\n",
    "                                        scores=scores_list,\n",
    "                                        num_document_passes=1, \n",
    "                                        seed=artm_seed)\n",
    "    level1.initialize(dictionary=dictionary)\n",
    "\n",
    "    # Обучим модель второго уровня\n",
    "    # TODO: подобрать количество итераций\n",
    "    level1.fit_offline(batch_vectorizer, num_collection_passes=2)\n",
    "\n",
    "    # Итеративное разреживание детей тем 0 уровня\n",
    "    threshold = 0.05\n",
    "    psi1 = level1.get_psi()\n",
    "    for tau in np.arange(0.1, 0.6, 0.1):\n",
    "        child_topics = []\n",
    "        for t in range(len(level0.topic_names)):\n",
    "            child_topics.append([])\n",
    "            for s, topic_name1 in enumerate(level1.topic_names):\n",
    "                if psi1.values[s, t] > threshold:\n",
    "                    child_topics[t].append(topic_name1)\n",
    "        # Разреживание детей каждой темы 0 уровня между собой\n",
    "        for i in range(len(level0.topic_names)):\n",
    "            topics_list = child_topics[i]\n",
    "            level1.regularizers.add(artm.SmoothSparseThetaRegularizer(\n",
    "                                    name=\"SPThetaReg_%d\" % i,\n",
    "                                    topic_names=topics_list, \n",
    "                                    tau=-tau * len(topics_list) ** 3), overwrite=True)\n",
    "        # Дообучим модель второго уровня\n",
    "        # TODO: подобрать количество итераций\n",
    "        level1.fit_offline(batch_vectorizer, num_collection_passes=1)\n",
    "\n",
    "    coherence = level1.score_tracker[\"Top10Tokens\"].average_coherence[-1]\n",
    "    return level1, coherence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Level 0 coherence: 0.978299, t=10\n",
      "Level 0 coherence: 0.888031, t=20\n",
      "Level 0 coherence: 0.890127, t=30\n",
      "Level 0 coherence: 0.898090, t=40\n",
      "Level 0 coherence: 0.917293, t=50\n",
      "Level 0 coherence: 0.893182, t=60\n",
      "Level 0 coherence: 0.875464, t=70\n",
      "Level 0 coherence: 0.859857, t=80\n",
      "Level 0 coherence: 0.852484, t=90\n",
      "Level 0 coherence: 0.865805, t=100\n",
      "Level 0 coherence: 0.852112, t=110\n",
      "Level 0 coherence: 0.845641, t=120\n",
      "Level 0 coherence: 0.831554, t=130\n",
      "Level 0 coherence: 0.840733, t=140\n",
      "Level 0 coherence: 0.828020, t=150\n",
      "Level 0 coherence: 0.815448, t=160\n",
      "Level 0 coherence: 0.814038, t=170\n",
      "Level 0 coherence: 0.831102, t=180\n",
      "Level 0 coherence: 0.818743, t=190\n",
      "Level 0 coherence: 0.810326, t=200\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception ignored in: <generator object __iter__ at 0x7ff0c5ab30a0>\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/anton/ML-Env/.py3/lib/python3.5/site-packages/tqdm/_tqdm_notebook.py\", line 194, in __iter__\n",
      "    self.sp(bar_style='danger')\n",
      "AttributeError: 'tqdm_notebook' object has no attribute 'sp'\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-69-00ee15d01480>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mget_ipython\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_cell_magic\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'time'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m''\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'\\nmax_batch_size = max(map(len, batches))\\n\\nvocab_set = set()\\nprev_phi = None\\n\\nall_chars = list(map(chr, range(ord(\\'a\\'), ord(\\'z\\') + 1)))\\nartm_batch_names_iter = itertools.product(*([all_chars] * 6))\\nbatch_names = [\"\".join(next(artm_batch_names_iter)) for i in range(len(batches))]\\n\\n# Удалим содержимое директории с ARTM батчами\\nfor fname in glob.glob(artm_batch_path + \"/*\"):\\n    os.remove(fname)\\n\\n# Создадим файлы с ARTM vocabulary\\nopen(artm_vocab_path, \"w\").close()\\n\\nfor batch_name, batch in zip(batch_names, batches):\\n    batch_vocab = []\\n\\n    # Запишем документы, словарь и cooc-словарь из батча в файлы\\n    with open(vw_path, \"w\") as vw_f, open(artm_vocab_path, \"a\") as vocab_f:\\n        for doc in map(get_document, batch):\\n            doc_id = doc[\"_id\"]\\n            modalities_str = []\\n            for mod_name, mod in doc[\"modalities\"].items():\\n                if mod_name in modalities_to_use:\\n                    modalities_str.append(\"|%s %s\" % (mod_name, \" \".join(mod)))\\n                    for token in mod:\\n                        token = token.replace(\" \", \"_\")\\n                        vocab_entry = (token, mod_name)\\n                        if vocab_entry not in vocab_set:\\n                            vocab_set.add(vocab_entry)\\n                            batch_vocab.append(vocab_entry)\\n            vw_f.write(\"%s %s\\\\n\" % (doc_id, \" \".join(modalities_str)))\\n        for vocab_entry in batch_vocab:\\n            vocab_f.write(\"%s %s\\\\n\" % vocab_entry)\\n\\n    # Создадим ARTM батч по нашему батчу\\n    tmp_batch_vectorizer = artm.BatchVectorizer(data_format=\"vowpal_wabbit\", data_path=vw_path,\\n                                                batch_size=max_batch_size, target_folder=tmp_artm_batch_path,\\n                                                gather_dictionary=False)\\n    # Хак: переименуем ARTM батч во временное имя,\\n    # чтобы не мешать созданию других ARTM батчей\\n    os.rename(\"%s/aaaaaa.batch\" % tmp_artm_batch_path,\\n              \"%s/%s.batch\"     % (artm_batch_path, batch_name))\\n    \\n    # Создадим ARTM dictionary\\n    dictionary = artm.Dictionary(\"dictionary\")\\n    dictionary.gather(artm_batch_path, vocab_file_path=artm_vocab_path,\\n                      cooc_file_path=artm_cooc_path, symmetric_cooc_values=True)\\n\\n    # Загрузим созданные на данный момент ARTM батчи\\n    batch_vectorizer = artm.BatchVectorizer(data_format=\"batches\", data_path=artm_batch_path,\\n                                            gather_dictionary=False)\\n\\n    # Метрики качества\\n    scores_list = []\\n    scores_list.append(artm.PerplexityScore(name=\"PerplexityScore\", class_ids=[\"text\"]))\\n    scores_list.append(artm.TopTokensScore(name=\"Top10Tokens\", class_id=\"text\", num_tokens=10,\\n                                           dictionary=dictionary))\\n\\n    for t in range(10, 401, 10):\\n        # Названия тем\\n        norm_topic_names = [\"topic_%d\" % i for i in range(0, t)]\\n        background_names = [\"background_%d\" % i for i in range(0, 1)]\\n        topic_names0 = norm_topic_names + background_names\\n\\n        # Регуляризаторы\\n        regularizers_list0 = []\\n        regularizers_list0.append(artm.DecorrelatorPhiRegularizer(name=\"DecorrPhiReg\",\\n                                                                  topic_names=norm_topic_names,\\n                                                                  tau=100000))\\n        regularizers_list0.append(artm.SmoothSparseThetaRegularizer(name=\"SPPhiTagRegBackground\",\\n                                                                   topic_names=background_names,\\n                                                                   tau=100))\\n\\n        level0 = artm.ARTM(topic_names=topic_names0, class_ids=class_ids0,\\n                           regularizers=regularizers_list0, scores=scores_list,\\n                           cache_theta=False, theta_columns_naming=\"title\",\\n                           seed=artm_seed)\\n        level0.initialize(dictionary=dictionary)\\n    \\n        # Инициализиуем матрицу Фи значениями с предыдущей итерации\\n        if prev_phi is not None:\\n            meta_info, phi_ref = level0.master.attach_model(level0.model_pwt)\\n            print(prev_phi.shape, phi_ref.shape)\\n            for i, j in itertools.product(*map(range, prev_phi.shape)):\\n                phi_ref[i, j] = prev_phi[i, j]\\n\\n        # Обучим модель первого уровня\\n        # TODO: подобрать количество итераций\\n        level0.fit_offline(batch_vectorizer, num_collection_passes=3)\\n        coherence = level0.score_tracker[\"Top10Tokens\"].average_coherence[-1]\\n        print(\"Level 0 coherence: %.6f, t=%d\" % (coherence, t))\\n\\n    # Сохраним матрицу Фи с текущей итерации\\n    prev_phi = level0.get_phi().values\\n    break'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/nix/store/ii1ni95z65zip6yfzvb0km4a59ihdn04-python3.5-ipython-5.3.0/lib/python3.5/site-packages/IPython/core/interactiveshell.py\u001b[0m in \u001b[0;36mrun_cell_magic\u001b[0;34m(self, magic_name, line, cell)\u001b[0m\n\u001b[1;32m   2113\u001b[0m             \u001b[0mmagic_arg_s\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvar_expand\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mline\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstack_depth\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2114\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuiltin_trap\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2115\u001b[0;31m                 \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmagic_arg_s\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcell\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2116\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2117\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<decorator-gen-59>\u001b[0m in \u001b[0;36mtime\u001b[0;34m(self, line, cell, local_ns)\u001b[0m\n",
      "\u001b[0;32m/nix/store/ii1ni95z65zip6yfzvb0km4a59ihdn04-python3.5-ipython-5.3.0/lib/python3.5/site-packages/IPython/core/magic.py\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(f, *a, **k)\u001b[0m\n\u001b[1;32m    186\u001b[0m     \u001b[0;31m# but it's overkill for just that one bit of state.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    187\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mmagic_deco\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 188\u001b[0;31m         \u001b[0mcall\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mlambda\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    189\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    190\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcallable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/nix/store/ii1ni95z65zip6yfzvb0km4a59ihdn04-python3.5-ipython-5.3.0/lib/python3.5/site-packages/IPython/core/magics/execution.py\u001b[0m in \u001b[0;36mtime\u001b[0;34m(self, line, cell, local_ns)\u001b[0m\n\u001b[1;32m   1183\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1184\u001b[0m             \u001b[0mst\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclock2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1185\u001b[0;31m             \u001b[0mexec\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mglob\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlocal_ns\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1186\u001b[0m             \u001b[0mend\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclock2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1187\u001b[0m             \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<timed exec>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m/nix/store/r19g3kqd961pldsda2rawx7i45k9m802-python3.5-bigartm/lib/python3.5/site-packages/artm/artm_model.py\u001b[0m in \u001b[0;36mfit_offline\u001b[0;34m(self, batch_vectorizer, num_collection_passes)\u001b[0m\n\u001b[1;32m    502\u001b[0m                     self._pool.apply_async(func=self.master.fit_offline,\n\u001b[1;32m    503\u001b[0m                                            args=(batches_list, batch_vectorizer.weights, 1, None)),\n\u001b[0;32m--> 504\u001b[0;31m                     len(batches_list))\n\u001b[0m\u001b[1;32m    505\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    506\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscores\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/nix/store/r19g3kqd961pldsda2rawx7i45k9m802-python3.5-bigartm/lib/python3.5/site-packages/artm/artm_model.py\u001b[0m in \u001b[0;36m_wait_for_batches_processed\u001b[0;34m(self, async_result, num_batches)\u001b[0m\n\u001b[1;32m    464\u001b[0m                 \u001b[0mprevious_num_batches\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    465\u001b[0m                 \u001b[0;32mwhile\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0masync_result\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mready\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 466\u001b[0;31m                     \u001b[0masync_result\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    467\u001b[0m                     current_num_batches = self.master.get_score(\n\u001b[1;32m    468\u001b[0m                         score_name='^^^ItemsProcessedScore^^^').num_batches\n",
      "\u001b[0;32m/nix/store/76rgpkm1jqgqwz4bjnk8y37z29q82zgy-python3-3.5.3/lib/python3.5/multiprocessing/pool.py\u001b[0m in \u001b[0;36mwait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    597\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    598\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 599\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_event\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    600\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    601\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/nix/store/76rgpkm1jqgqwz4bjnk8y37z29q82zgy-python3-3.5.3/lib/python3.5/threading.py\u001b[0m in \u001b[0;36mwait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    547\u001b[0m             \u001b[0msignaled\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_flag\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    548\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0msignaled\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 549\u001b[0;31m                 \u001b[0msignaled\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_cond\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    550\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0msignaled\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    551\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/nix/store/76rgpkm1jqgqwz4bjnk8y37z29q82zgy-python3-3.5.3/lib/python3.5/threading.py\u001b[0m in \u001b[0;36mwait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    295\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    296\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mtimeout\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 297\u001b[0;31m                     \u001b[0mgotit\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mwaiter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0macquire\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    298\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    299\u001b[0m                     \u001b[0mgotit\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mwaiter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0macquire\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "max_batch_size = max(map(len, batches))\n",
    "\n",
    "vocab_set = set()\n",
    "prev_phi = None\n",
    "\n",
    "all_chars = list(map(chr, range(ord('a'), ord('z') + 1)))\n",
    "artm_batch_names_iter = itertools.product(*([all_chars] * 6))\n",
    "batch_names = [\"\".join(next(artm_batch_names_iter)) for i in range(len(batches))]\n",
    "\n",
    "# Удалим содержимое директории с ARTM батчами\n",
    "for fname in glob.glob(artm_batch_path + \"/*\"):\n",
    "    os.remove(fname)\n",
    "\n",
    "# Создадим файлы с ARTM vocabulary\n",
    "open(artm_vocab_path, \"w\").close()\n",
    "\n",
    "for batch_name, batch in zip(batch_names, batches):\n",
    "    batch_vocab = []\n",
    "\n",
    "    # Запишем документы, словарь и cooc-словарь из батча в файлы\n",
    "    with open(vw_path, \"w\") as vw_f, open(artm_vocab_path, \"a\") as vocab_f:\n",
    "        for doc in map(get_document, batch):\n",
    "            doc_id = doc[\"_id\"]\n",
    "            modalities_str = []\n",
    "            for mod_name, mod in doc[\"modalities\"].items():\n",
    "                if mod_name in modalities_to_use:\n",
    "                    modalities_str.append(\"|%s %s\" % (mod_name, \" \".join(mod)))\n",
    "                    for token in mod:\n",
    "                        token = token.replace(\" \", \"_\")\n",
    "                        vocab_entry = (token, mod_name)\n",
    "                        if vocab_entry not in vocab_set:\n",
    "                            vocab_set.add(vocab_entry)\n",
    "                            batch_vocab.append(vocab_entry)\n",
    "            vw_f.write(\"%s %s\\n\" % (doc_id, \" \".join(modalities_str)))\n",
    "        for vocab_entry in batch_vocab:\n",
    "            vocab_f.write(\"%s %s\\n\" % vocab_entry)\n",
    "\n",
    "    # Создадим ARTM батч по нашему батчу\n",
    "    tmp_batch_vectorizer = artm.BatchVectorizer(data_format=\"vowpal_wabbit\", data_path=vw_path,\n",
    "                                                batch_size=max_batch_size, target_folder=tmp_artm_batch_path,\n",
    "                                                gather_dictionary=False)\n",
    "    # Хак: переименуем ARTM батч во временное имя,\n",
    "    # чтобы не мешать созданию других ARTM батчей\n",
    "    os.rename(\"%s/aaaaaa.batch\" % tmp_artm_batch_path,\n",
    "              \"%s/%s.batch\"     % (artm_batch_path, batch_name))\n",
    "    \n",
    "    # Создадим ARTM dictionary\n",
    "    dictionary = artm.Dictionary(\"dictionary\")\n",
    "    dictionary.gather(artm_batch_path, vocab_file_path=artm_vocab_path,\n",
    "                      cooc_file_path=artm_cooc_path, symmetric_cooc_values=True)\n",
    "\n",
    "    # Загрузим созданные на данный момент ARTM батчи\n",
    "    batch_vectorizer = artm.BatchVectorizer(data_format=\"batches\", data_path=artm_batch_path,\n",
    "                                            gather_dictionary=False)\n",
    "\n",
    "    # Метрики качества\n",
    "    scores_list = []\n",
    "    scores_list.append(artm.PerplexityScore(name=\"PerplexityScore\", class_ids=[\"text\"]))\n",
    "    scores_list.append(artm.TopTokensScore(name=\"Top10Tokens\", class_id=\"text\", num_tokens=10,\n",
    "                                           dictionary=dictionary))\n",
    "\n",
    "    for t in range(10, 401, 10):\n",
    "        # Названия тем\n",
    "        norm_topic_names = [\"topic_%d\" % i for i in range(0, t)]\n",
    "        background_names = [\"background_%d\" % i for i in range(0, 1)]\n",
    "        topic_names0 = norm_topic_names + background_names\n",
    "\n",
    "        # Регуляризаторы\n",
    "        regularizers_list0 = []\n",
    "        regularizers_list0.append(artm.DecorrelatorPhiRegularizer(name=\"DecorrPhiReg\",\n",
    "                                                                  topic_names=norm_topic_names,\n",
    "                                                                  tau=100000))\n",
    "        regularizers_list0.append(artm.SmoothSparseThetaRegularizer(name=\"SPPhiTagRegBackground\",\n",
    "                                                                   topic_names=background_names,\n",
    "                                                                   tau=100))\n",
    "\n",
    "        level0 = artm.ARTM(topic_names=topic_names0, class_ids=class_ids0,\n",
    "                           regularizers=regularizers_list0, scores=scores_list,\n",
    "                           cache_theta=False, theta_columns_naming=\"title\",\n",
    "                           seed=artm_seed)\n",
    "        level0.initialize(dictionary=dictionary)\n",
    "    \n",
    "        # Инициализиуем матрицу Фи значениями с предыдущей итерации\n",
    "        if prev_phi is not None:\n",
    "            meta_info, phi_ref = level0.master.attach_model(level0.model_pwt)\n",
    "            print(prev_phi.shape, phi_ref.shape)\n",
    "            for i, j in itertools.product(*map(range, prev_phi.shape)):\n",
    "                phi_ref[i, j] = prev_phi[i, j]\n",
    "\n",
    "        # Обучим модель первого уровня\n",
    "        # TODO: подобрать количество итераций\n",
    "        level0.fit_offline(batch_vectorizer, num_collection_passes=3)\n",
    "        coherence = level0.score_tracker[\"Top10Tokens\"].average_coherence[-1]\n",
    "        print(\"Level 0 coherence: %.6f, t=%d\" % (coherence, t))\n",
    "\n",
    "    # Сохраним матрицу Фи с текущей итерации\n",
    "    prev_phi = level0.get_phi().values\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "s = \"\"\"Level 0 coherence: 0.978299, t=10\n",
    "Level 0 coherence: 0.888031, t=20\n",
    "Level 0 coherence: 0.890127, t=30\n",
    "Level 0 coherence: 0.898090, t=40\n",
    "Level 0 coherence: 0.917293, t=50\n",
    "Level 0 coherence: 0.893182, t=60\n",
    "Level 0 coherence: 0.875464, t=70\n",
    "Level 0 coherence: 0.859857, t=80\n",
    "Level 0 coherence: 0.852484, t=90\n",
    "Level 0 coherence: 0.865805, t=100\n",
    "Level 0 coherence: 0.852112, t=110\n",
    "Level 0 coherence: 0.845641, t=120\n",
    "Level 0 coherence: 0.831554, t=130\n",
    "Level 0 coherence: 0.840733, t=140\n",
    "Level 0 coherence: 0.828020, t=150\n",
    "Level 0 coherence: 0.815448, t=160\n",
    "Level 0 coherence: 0.814038, t=170\n",
    "Level 0 coherence: 0.831102, t=180\n",
    "Level 0 coherence: 0.818743, t=190\n",
    "Level 0 coherence: 0.810326, t=200\"\"\"\n",
    "\n",
    "ys, xs = zip(*map(lambda r: tuple(map(float, r.split(\": \", 1)[1].split(\", t=\"))), s.split(\"\\n\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7ff096645668>]"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYQAAAD8CAYAAAB3u9PLAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n/h9wquT1IyuyzgAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7ff0a54b8a90>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(xs, ys)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "```\n",
    "top-10 tokens\n",
    "Level 0 coherence: 0.862778, t=10\n",
    "Level 0 coherence: 1.159598, t=20\n",
    "Level 0 coherence: 1.267626, t=30\n",
    "Level 0 coherence: 1.468547, t=40\n",
    "Level 0 coherence: 1.530683, t=50\n",
    "Level 0 coherence: 1.565759, t=60\n",
    "Level 0 coherence: 1.668325, t=70\n",
    "Level 0 coherence: 1.722331, t=80\n",
    "Level 0 coherence: 1.779689, t=90\n",
    "Level 0 coherence: 1.918866, t=100\n",
    "Level 0 coherence: 1.924216, t=110\n",
    "Level 0 coherence: 1.980874, t=120\n",
    "Level 0 coherence: 2.079673, t=130\n",
    "Level 0 coherence: 2.093757, t=140\n",
    "Level 0 coherence: 2.173409, t=150\n",
    "Level 0 coherence: 2.174175, t=160\n",
    "Level 0 coherence: 2.205918, t=170\n",
    "Level 0 coherence: 2.275994, t=180\n",
    "Level 0 coherence: 2.277267, t=190\n",
    "Level 0 coherence: 2.332946, t=200\n",
    "Level 0 coherence: 2.346622, t=210\n",
    "Level 0 coherence: 2.283602, t=220\n",
    "Level 0 coherence: 2.324260, t=230\n",
    "Level 0 coherence: 2.317958, t=240\n",
    "Level 0 coherence: 2.315762, t=250\n",
    "Level 0 coherence: 2.279474, t=260\n",
    "Level 0 coherence: 2.309187, t=270\n",
    "Level 0 coherence: 2.252727, t=280\n",
    "Level 0 coherence: 2.182547, t=290\n",
    "Level 0 coherence: 2.200595, t=300\n",
    "Level 0 coherence: 2.178718, t=310\n",
    "Level 0 coherence: 2.021726, t=320\n",
    "Level 0 coherence: 2.035341, t=330\n",
    "Level 0 coherence: 2.026407, t=340\n",
    "Level 0 coherence: 1.923857, t=350\n",
    "Level 0 coherence: 1.828151, t=360\n",
    "Level 0 coherence: 1.822736, t=370\n",
    "Level 0 coherence: 1.762612, t=380\n",
    "Level 0 coherence: 1.778792, t=390\n",
    "Level 0 coherence: 1.736851, t=400\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "```\n",
    "top-20 tokens\n",
    "Level 0 coherence: 0.529625, t=10\n",
    "Level 0 coherence: 0.689338, t=20\n",
    "Level 0 coherence: 0.799413, t=30\n",
    "Level 0 coherence: 0.945375, t=40\n",
    "Level 0 coherence: 1.023195, t=50\n",
    "Level 0 coherence: 1.064831, t=60\n",
    "Level 0 coherence: 1.133953, t=70\n",
    "Level 0 coherence: 1.254620, t=80\n",
    "Level 0 coherence: 1.247764, t=90\n",
    "Level 0 coherence: 1.346262, t=100\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "**NB**: в конце провести эксперимент на перемешанном clf_output -- гипотеза о независимости качества от порядка вливания документов в ТМ."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  },
  "latex_envs": {
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 0
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
